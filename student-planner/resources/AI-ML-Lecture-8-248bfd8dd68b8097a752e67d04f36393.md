
## Quality of Data
There are 3Vs of big data

1. **Variety**
2. **Velocity**
3. **Volume**

Success of ML projects heavily depend on these factors
Data quality can be defined as "_fitness of use_"


---


## Characteristics of Data Quality

1. **Completeness**
	1. All of the required data is present
	2. Meets objectives of project
2. **Accuracy**
	1. Does data reflect reality
3. **Timeliness**
	1. Data being available when it is needed for decision making
	2. Data is latest
4. **Consistency**
	1. Format should be consistent across platform (example birthdate format)
5. **Validity**
	1. Pre-defined limits, expectations and standards
6. **Uniqueness**
	1. Not be be redundant

---


## Feature Engineering

- **A feature is a numeric representation of raw data**
- **Features can be represented as columns**
- Number of features is important
	- If less features, unable to perform the ultimate task
	- If more features, many become irrelevant and model is expensive and tricky to train

_Feature Engineering: It is the process of extracting and organizing important features from raw data in such a way that it fits the purpose of the machine learning model._


![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/cb8bfd8d-d68b-81fa-ac15-000328a0aab4/223459bc-d9dc-46c4-a449-0bd7aa5c7efd/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB4667U5XHH2P%2F20250815%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250815T064756Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEA8aCXVzLXdlc3QtMiJIMEYCIQCIm21uWtPayhqk8mTxIOgn%2Fuc%2FfJ8FsrejLXHailEn9gIhANB06xT24WxLcVrGV1reus14DfMSPJLcz3%2FyAQkS6hTDKv8DCFcQABoMNjM3NDIzMTgzODA1Igw6Yg2ZqsFcSe8qK1Mq3AOQeg4b0AKRQ01hf4jG21l7liRNnAcfoTrc9cKOMs7SCO%2FrrGOzfxEXsu%2BMzFl9WYHRHqE0w3PR7BKYX4vgCikO%2FhCgc1D4Czz3JgrYjO%2BP15PisIzIG49i022zVQJJwXSKenuGf2rutILA2JoXoYFn2gOxdDc4sRQ1RE5phoo7bBSHV3vH9xeEqPRIZXasaRHKDCPhZHVc6Xn2owBlkZkMUP574WgdaP3%2FKFeG0vDJ4IGnzL6p30YIvT%2FWV1DsEroeovcfBSBH%2FSQQDnli4DhXnEQJYVGnDM8ReacqUT0Ags5nUkJqmWT%2BpLZQunv8PlwKlizmQddkZ1tQtZorhmRUy%2FWZVr6xMvVc8Dc9XX8XwRTpxMexuk9PPRuCH0tvhuKV%2F%2BHpdRV7JZqWStv%2BKwu1PSiHfcxDEdugzSF8qfDnWEZGOys8wD8c0rCkGPzM1v72NAvkIfOnUkquDfuHQZB7Gt9pAfPhngunkU9sZ5JiEKdwbYcfBAiDpWMlzFHYxfTvJDfo29YCOQteXQSxZuNLhivKonFqPxNVPBxW1n9DqrR8xeDyW5dPmM%2Bg4LWcYeKBAEvFjVotGNta8JRruxbi9KKtvs5Oe9M21BaSHSKP7Hdl4q%2F3DgTEllbYojD2o%2FvEBjqkARNTVJxgNL498N0qO6l3dvDx83X4X0U3wzhC%2FCB0KhBNmSJ6oeSYg2W06AkwXHAKpaiLJQDnupemN8Wk5vJhKNmc00SygmJIdbvtlIxcirH69SkZsElnmOXMXcYh9Nh5L0WJc99ZFgpe9lLq68ZGORGID28sIkuvyPi53IUTRNHDUOxWoYyUU5xMGapVnbYZlD%2BXWh1hElVHjYwhgYrpI8b%2B%2FHDZ&X-Amz-Signature=523ee5f39847ab1670f694a98996c701701c763fe7ba9e29d470e2868f31ec26&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject)


Has 2 goals:

1. Preparing proper input dataset
2. Improving performance

---


## Steps of Feature Engineering


### Feature Creation


Creating new variables / adding or removing some features


Example: Extracting day of the week from data


### Transformation


Transforms features from one representation to another 


Example: Log transformations, Min-Max scaling, Encoding, Standardization, etc.


Goal is to:

- To convert raw data into a suitable format
- Handle issues like skewed distributions, missing data, outliers

### Feature Extraction

- Extracting info from original features to create a new subspace
- Idea is to compress the data
- Example: PCA, LDA

### Exploratory data analysis

- Exploring its properties
- Create new hypotheses or find patterns in the data

### Benchmarks

- Benchmark model - A well-known interpretable model
- Benchmark dataset - A well known. well labelled and recognized dataset for a domain.

### Feature Selection

- Identifying and selecting the most relevant subset of features

---


## Difference between Feature Selection & Feature Extraction


| **Feature Selection**                                               | **Feature Extraction**                                       |
| ------------------------------------------------------------------- | ------------------------------------------------------------ |
| Selects a subset of the most relevant original features             | Creates new features by transforming the original features   |
| Does not transform features                                         | Transforms features into a lower-dimensional space           |
| Reduces dimensionality by eliminating redundant/irrelevant features | Doesn't keep original features intact                        |
| Original features remain interpretable                              | Creates entirely new features that may be less interpretable |


---


## FE Techniques


### Imputation

1. Variable Deletion (both numeric and categorical): dropping columns where 60% is NaN
2. Mean or median imputation: Replace with mean or median
3. Mode Imputation:
4. Assigning New Category
5. Predict Missing new values (both numeric and categorical)

### Outlier Handling

1. Removal
2. Replacing values
3. Capping

### Log Transform

1. Turn a skewed distribution into a normal or less-skewed distribution

### 

